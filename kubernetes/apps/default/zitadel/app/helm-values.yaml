# Default values for zitadel.
zitadel:
  # The ZITADEL config under configmapConfig is written to a Kubernetes ConfigMap
  # See all defaults here:
  # https://github.com/zitadel/zitadel/blob/main/cmd/defaults.yaml
  configmapConfig:
    # Log:
    #   Level: debug # ZITADEL_LOG_LEVEL
    #   Formatter:
    #     Format: text # ZITADEL_LOG_FORMATTER_FORMAT
    # Tracing:
    #   # Choose one in "otel", "google", "log" and "none"
    #   # Depending on the type there are different configuration options
    #   # for type 'otel' is used for standard [open telemetry](https://opentelemetry.io)
    #   # Fraction: 1.0
    #   # Endpoint: 'otel.collector.endpoint'
    #   # ServiceName: 'ZITADEL' # Name of the service in traces
    #   #
    #   # type 'log' or '' disables tracing
    #   #
    #   # for type 'google'
    #   # ProjectID: ''
    #   # Fraction: 1.0
    #   Type: otel # ZITADEL_TRACING_TYPE
    #   Fraction: 1.0 # ZITADEL_TRACING_FRACTION
    #   # The endpoint of the otel collector endpoint
    #   Endpoint: "" #ZITADEL_TRACING_ENDPOINT
    #   # The name of the service in traces
    #   ServiceName: "ZITADEL" #ZITADEL_TRACING_SERVICENAME

    ExternalPort:  443  # ZITADEL_EXTERNALPORT
    # ExternalDomain is the domain on which end users access ZITADEL.
    # Read more about external access: https://zitadel.com/docs/self-hosting/manage/custom-domain
    ExternalDomain: zitadel.dartus.fr # ZITADEL_EXTERNALDOMAIN
    # ExternalSecure specifies if ZITADEL is exposed externally using HTTPS or HTTP.
    # Read more about external access: https://zitadel.com/docs/self-hosting/manage/custom-domain
    ExternalSecure: true # ZITADEL_EXTERNALSECURE
    TLS:
      # If enabled, ZITADEL will serve all traffic over TLS (HTTPS and gRPC)
      # you must then also provide a private key and certificate to be used for the connection
      # either directly or by a path to the corresponding file
      Enabled: false # ZITADEL_TLS_ENABLED

    Database:
      # Postgres is the default database of ZITADEL
      postgres:
        Host: cnpg-cluster-rw.database.svc.cluster.local
        Port: 5432
        Database: zitadel
        MaxOpenConns: 20
        MaxIdleConns: 10
        MaxConnLifetime: 30m
        MaxConnIdleTime: 5m
        Options: "" # ZITADEL_DATABASE_POSTGRES_OPTIONS
        User:
          Username: vauban # ZITADEL_DATABASE_POSTGRES_USER_USERNAME
          Password: "" # ZITADEL_DATABASE_POSTGRES_USER_PASSWORD
          SSL:
            Mode: disable # ZITADEL_DATABASE_POSTGRES_USER_SSL_MODE
            RootCert: "" # ZITADEL_DATABASE_POSTGRES_USER_SSL_ROOTCERT
            Cert: "" # ZITADEL_DATABASE_POSTGRES_USER_SSL_CERT
            Key: "" # ZITADEL_DATABASE_POSTGRES_USER_SSL_KEY
    Machine:
      Identification:
        Hostname:
          Enabled: true
        Webhook:
          Enabled: false
    FirstInstance:
      Org:
        LoginClient:
          Machine:
            Username: 'login-client'
            Name: 'Automatically Initialized IAM Login Client'
          Pat.ExpirationDate: '2029-01-01T00:00:00Z'

  # The ZITADEL config under secretConfig is written to a Kubernetes Secret
  # See all defaults here:
  # https://github.com/zitadel/zitadel/blob/main/cmd/defaults.yaml
  secretConfig:

  # Annotations set on secretConfig secret
  secretConfigAnnotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

  # Reference the name of a secret that contains ZITADEL configuration.
  configSecretName:
  # The key under which the ZITADEL configuration is located in the secret.
  configSecretKey: config-yaml

  # ZITADEL uses the masterkey for symmetric encryption.
  # You can generate it for example with tr -dc A-Za-z0-9 </dev/urandom | head -c 32
  masterkey: ""
  # Reference the name of the secret that contains the masterkey. The key should be named "masterkey".
  # Note: Either zitadel.masterkey or zitadel.masterkeySecretName must be set
  masterkeySecretName: "zitadel-masterkey"

  # Number of old ReplicaSets to retain for rollback purposes
  # Set to 0 to not keep any old ReplicaSets
  revisionHistoryLimit: 10

  # Annotations set on masterkey secret
  masterkeyAnnotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

  # The CA Certificate needed for establishing secure database connections
  dbSslCaCrt: ""

  # Annotations set on database SSL CA certificate secret
  dbSslCaCrtAnnotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

  # The Secret containing the CA certificate at key ca.crt needed for establishing secure database connections
  dbSslCaCrtSecret: "cnpg-server-cert"

  # The db admins secret containing the client certificate and key at tls.crt and tls.key needed for establishing secure database connections
  dbSslAdminCrtSecret: "cnpg-server-cert"

  # The db users secret containing the client certificate and key at tls.crt and tls.key needed for establishing secure database connections
  dbSslUserCrtSecret: "cnpg-client-cert"

  # The Secret containing the certificate at key tls.crt and tls.key for listening on HTTPS
  serverSslCrtSecret: "zitadel.self-tls"

  # Generate a self-signed certificate using an init container
  # This will also mount the generated files to /etc/tls/ so that you can reference them in the pod.
  # E.G. KeyPath: /etc/tls/tls.key CertPath: /etc/tls/tls.crt
  # By default, the SAN DNS names include, localhost, the POD IP address and the POD name. You may include one more by using additionalDnsName like "my.zitadel.fqdn".
  selfSignedCert:
    enabled: false
    additionalDnsName:

  # Enabling this will create a debug pod that can be used to inspect the ZITADEL configuration and run zitadel commands using the zitadel binary.
  # This is useful for debugging and troubleshooting.
  # After the debug pod is created, you can open a shell within the pod.
  # See more instructions by printing the pods logs using kubectl logs [pod name].
  debug:
    enabled: false
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-weight: "1"
    initContainers: []
    extraContainers: []

  # initContainers allow you to add any init containers you wish to use globally.
  # Additionally, they follow the same structure as extraContainers
  initContainers: []
  # extraContainers allows you to add any sidecar containers you wish to use globally.
  # Currently this is the Zitadel Deployment, Setup Job**, Init Job** and debug_replicaset**  **If Enabled
  extraContainers: []
    # # Example; You wish to deploy a cloud-sql-proxy sidecar to all deployments:
    # - name: cloud-sql-proxy
    #   image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.14.1
    #   command:
    #     - /cloud-sql-proxy
    #   args:
    #     - my-project:my-region:my-instance
    #     - --port=5432
    #     - --auto-iam-authn
    #     - --health-check
    #     - "--http-address=0.0.0.0"
    #   ports:
    #     - containerPort: 5432
    #   startupProbe:
    #     httpGet:
    #       path: /startup
    #       port: 9090
    #     periodSeconds: 1
    #     timeoutSeconds: 5
    #   livenessProbe:
    #     httpGet:
    #       path: /liveness
    #       port: 9090
    #     initialDelaySeconds: 0
    #     periodSeconds: 60
    #     timeoutSeconds: 30
    #     failureThreshold: 5
    #   securityContext:
    #     runAsNonRoot: true
    #     readOnlyRootFilesystem: true
    #     allowPrivilegeEscalation: false
    #   lifecycle:
    #     postStart:
    #       exec:
    #         command: ["/cloud-sql-proxy", "wait"]

login:
  enabled: true
  # customConfigmapConfig defaults to this:
  # ZITADEL_SERVICE_USER_TOKEN_FILE="/login-client/pat"
  # ZITADEL_API_URL="http://{{ include "zitadel.fullname" . }}:{{ .Values.service.port }}"
  # CUSTOM_REQUEST_HEADERS="Host:{{ .Values.zitadel.configmapConfig.ExternalDomain }}"
  customConfigmapConfig:
  # To deploy zitadel multiple times in the same namespace, use a loginClientSecretPrefix.
  # To mount it, also change the referenced secretName for the login client to "{loginClientSecretPrefix}login-client"
  loginClientSecretPrefix:
  extraVolumeMounts:
  - name: login-client
    mountPath: /login-client
    readOnly: true
  extraVolumes:
  - name: login-client
    secret:
      defaultMode: 444
      # Adjust this if you want to reference a different secret for the login client PAT.
      secretName: login-client
  replicaCount: 3
  initContainers: []
  extraContainers: []
  image:
    repository: ghcr.io/zitadel/zitadel-login
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: ""
  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""
  podAnnotations: {}
  podAdditionalLabels: {}
  # Annotations to add to the configMap
  configMap:
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "0"
  readinessProbe:
    enabled: true
    initialDelaySeconds: 0
    periodSeconds: 5
    failureThreshold: 3
  livenessProbe:
    enabled: true
    initialDelaySeconds: 0
    periodSeconds: 5
    failureThreshold: 3
  startupProbe:
    enabled: false
    periodSeconds: 1
    failureThreshold: 30
  service:
    type: ClusterIP
    # If service type is "ClusterIP", this can optionally be set to a fixed IP address.
    clusterIP: ""
    # If service type is "LoadBalancer", this can optionally be set to either "Cluster" or "Local"
    externalTrafficPolicy: ""
    port: 3000
    protocol: http
    appProtocol: kubernetes.io/http
    annotations: {}
    labels: {}
    scheme: HTTP
  ingress:
    enabled: false
    className: ""
    annotations: {}
    hosts:
      - paths:
          - path: /ui/v2/login
            pathType: Prefix
    tls: []
  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations:
      helm.sh/hook: pre-install,pre-upgrade
      helm.sh/hook-delete-policy: before-hook-creation
      helm.sh/hook-weight: "0"
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    readOnlyRootFilesystem: true
    privileged: false
  resources: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  topologySpreadConstraints: []
  revisionHistoryLimit: 10

replicaCount: 1

image:
  repository: ghcr.io/zitadel/zitadel
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Annotations to add to the deployment
annotations: {}

# Annotations to add to the configMap
configMap:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "0"
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podAdditionalLabels: {}

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  readOnlyRootFilesystem: true
  privileged: false

# Additional environment variables
env:
  - name: ZITADEL_DATABASE_POSTGRES_USER_PASSWORD
    valueFrom:
      secretKeyRef:
        name: vauban-secret
        key: password
  - name: ZITADEL_FIRSTINSTANCE_ORG_HUMAN_USERNAME
    valueFrom:
      secretKeyRef:
        name: smtp-secret
        key: gmail_username
  - name: ZITADEL_FIRSTINSTANCE_ORG_HUMAN_PASSWORD
    valueFrom:
      secretKeyRef:
        name: default-secret
        key: secpassword
  - name: ZITADEL_FIRSTINSTANCE_ORG_HUMAN_PASSWORDCHANGEREQUIRED
    value: "false"

  # - name: ZITADEL_TLS_KEY
  #   valueFrom:
  #     secretKeyRef:
  #       name: zitadel.self-tls
  #       key: tls.key
  # - name: ZITADEL_TLS_CERT
  #   valueFrom:
  #     secretKeyRef:
  #       name: zitadel.self-tls
  #       key: tls.crt
  # - name: ZITADEL_DEFAULTINSTANCE_SMTPCONFIGURATION_SMTP_USER
  #   valueFrom:
  #     secretKeyRef:
  #       name: smtp-secret
  #       key: gmail_username
  # - name: ZITADEL_DEFAULTINSTANCE_SMTPCONFIGURATION_FROM
  #   valueFrom:
  #     secretKeyRef:
  #       name: smtp-secret
  #       key: gmail_username
  # - name: ZITADEL_DEFAULTINSTANCE_SMTPCONFIGURATION_REPLYTOADDRESS
  #   valueFrom:
  #     secretKeyRef:
  #       name: smtp-secret
  #       key: gmail_username
  # - name: ZITADEL_DEFAULTINSTANCE_SMTPCONFIGURATION_SMTP_PASSWORD
  #   valueFrom:
  #     secretKeyRef:
  #       name: smtp-secret
  #       key: gmail_password


# Additional environment variables from the given secret name
# Zitadel can be configured using environment variables from a secret.
# Reference: https://zitadel.com/docs/self-hosting/manage/configure#configure-by-environment-variables
envVarsSecret: ""

service:
  type: ClusterIP
  # If service type is "ClusterIP", this can optionally be set to a fixed IP address.
  clusterIP: ""
  # If service type is "LoadBalancer", this can optionally be set to either "Cluster" or "Local"
  externalTrafficPolicy: ""
  port: 8080
  protocol: http2
  appProtocol: kubernetes.io/h2c
  annotations:
    traefik.ingress.kubernetes.io/service.serversscheme: h2c
  labels: {}
  scheme: HTTP

ingress:
  enabled: true
  className: "traefik-ingresses"
  annotations:
    hajimari.io/enable: "true"
    hajimari.io/group: "Media"
    hajimari.io/icon: "security"
    hajimari.io/appName: "zitadel"
    cert-manager.io/cluster-issuer: zerossl-prod
    acme.cert-manager.io/http01-edit-in-place: "false"

  hosts:
    - host: zitadel.dartus.fr
      paths:
        - path: /
          pathType: Prefix
  tls:
  - hosts:
      - zitadel.dartus.fr
    secretName: zitadel.dartus.fr-tls

resources: {}

nodeSelector: {}

tolerations: []

affinity: {}

topologySpreadConstraints: []

initJob:
  # Once ZITADEL is installed, the initJob can be disabled.
  enabled: true
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "1"
  resources: {}
  backoffLimit: 5
  activeDeadlineSeconds: 300
  initContainers: []
  extraContainers: []
  podAnnotations: {}
  podAdditionalLabels: {}
  # Available init commands :
  # "": initialize ZITADEL instance (without skip anything)
  # database: initialize only the database
  # grant: set ALL grant to user
  # user: initialize only the database user
  # zitadel: initialize ZITADEL internals (skip "create user" and "create database")
  command: "zitadel"

setupJob:
  annotations:
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "2"
  resources: {}
  backoffLimit: 5
  activeDeadlineSeconds: 300
  initContainers: []
  extraContainers: []
  podAnnotations: {}
  podAdditionalLabels: {}
  additionalArgs:
    - "--init-projections=true"
  machinekeyWriter:
    image:
      repository: bitnami/kubectl
      tag: ""
    resources: {}

readinessProbe:
  enabled: true
  initialDelaySeconds: 0
  periodSeconds: 5
  failureThreshold: 3

livenessProbe:
  enabled: true
  initialDelaySeconds: 0
  periodSeconds: 5
  failureThreshold: 3

startupProbe:
  enabled: true
  periodSeconds: 1
  failureThreshold: 30

metrics:
  enabled: false
  serviceMonitor:
    # If true, the chart creates a ServiceMonitor that is compatible with Prometheus Operator
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.ServiceMonitor.
    # The Prometheus community Helm chart installs this operator
    # https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#kube-prometheus-stack
    enabled: false
    honorLabels: false
    honorTimestamps: true

pdb:
  enabled: false
  # these values are used for the PDB and are mutally exclusive
  minAvailable: 1
  # maxUnavailable: 1
  annotations: {}

# extraContainers allows you to add any sidecar containers you wish to use in the Zitadel pod.
extraContainers: []

extraVolumes: []
  # - name: ca-certs
  #   secret:
  #     defaultMode: 420
  #     secretName: ca-certs

extraVolumeMounts: []
  # - name: ca-certs
  #   mountPath: /etc/ssl/certs/myca.pem
  #   subPath: myca.pem
  #   readOnly: true

# extraManifests allows you to add your own Kubernetes manifests
# You can use templating logic like {{ .Release.Namespace }} and {{ .Values.replicaCount }} as long as your manifest is a valid YAML
extraManifests: []
  # - apiVersion: v1
  #   kind: Secret
  #   metadata:
  #     name: {{ include "zitadel.fullname" . }}-my-secret
  #   stringData:
  #     key: value
  #   type: Opaque
